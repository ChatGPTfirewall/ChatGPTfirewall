"""
å¢å¼ºç‰ˆå‘é‡åŒ–å¤„ç† - å¸¦è¯¦ç»†æ—¥å¿—è®°å½•
===============================

åœ¨åŸæœ‰å‘é‡åŒ–æµç¨‹åŸºç¡€ä¸Šæ·»åŠ å®Œæ•´çš„è¿›åº¦è·Ÿè¸ªå’Œé”™è¯¯å¤„ç†
"""

import logging
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from dataclasses import dataclass

# å¯¼å…¥æ—¥å¿—é…ç½®
from logging_config import setup_logging, log_performance, log_errors

# è®¾ç½®æ—¥å¿—
loggers = setup_logging()
vectorization_logger = loggers['vectorization']
qdrant_logger = loggers['qdrant']


@dataclass
class VectorizationResult:
    """å‘é‡åŒ–ç»“æœæ•°æ®ç±»"""
    success: bool
    document_id: Optional[int] = None
    sections_created: int = 0
    vectors_stored: int = 0
    processing_time: float = 0.0
    error: Optional[str] = None
    steps: List[str] = None
    
    def __post_init__(self):
        if self.steps is None:
            self.steps = []


class EnhancedVectorizer:
    """å¢å¼ºç‰ˆå‘é‡åŒ–å¤„ç†å™¨ - å¸¦å®Œæ•´æ—¥å¿—è®°å½•"""
    
    def __init__(self):
        self.vectorization_session_id = None
        self.models_loaded = False
        self._load_models()
    
    @log_performance("vectorization")
    def _load_models(self):
        """åŠ è½½å‘é‡åŒ–æ¨¡å‹"""
        vectorization_logger.info("ğŸ¤– å¼€å§‹åŠ è½½å‘é‡åŒ–æ¨¡å‹")
        
        try:
            # åŠ è½½spaCyæ¨¡å‹
            import spacy
            
            self.nlp_de = None
            self.nlp_en = None
            
            try:
                self.nlp_de = spacy.load("de_core_news_sm")
                vectorization_logger.info("âœ… å¾·è¯­spaCyæ¨¡å‹åŠ è½½æˆåŠŸ")
            except OSError:
                vectorization_logger.warning("âš ï¸ å¾·è¯­spaCyæ¨¡å‹æœªæ‰¾åˆ°")
            
            try:
                self.nlp_en = spacy.load("en_core_web_sm")
                vectorization_logger.info("âœ… è‹±è¯­spaCyæ¨¡å‹åŠ è½½æˆåŠŸ")
            except OSError:
                vectorization_logger.warning("âš ï¸ è‹±è¯­spaCyæ¨¡å‹æœªæ‰¾åˆ°")
            
            # åŠ è½½SentenceTransformer
            from sentence_transformers import SentenceTransformer
            
            model_name = "paraphrase-multilingual-MiniLM-L12-v2"
            vectorization_logger.info(f"ğŸš€ åŠ è½½SentenceTransformeræ¨¡å‹: {model_name}")
            
            self.sentence_model = SentenceTransformer(model_name)
            vectorization_logger.info("âœ… SentenceTransformeræ¨¡å‹åŠ è½½æˆåŠŸ")
            
            self.models_loaded = True
            vectorization_logger.info("ğŸ‰ æ‰€æœ‰å‘é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            vectorization_logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            import traceback
            vectorization_logger.error(f"å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
            self.models_loaded = False
    
    @log_performance("vectorization", log_args=True)
    def vectorize_document(self, document, language: str = 'de') -> VectorizationResult:
        """å‘é‡åŒ–æ–‡æ¡£ - ä¸»å…¥å£å‡½æ•°"""
        
        # ç”Ÿæˆå‘é‡åŒ–ä¼šè¯ID
        self.vectorization_session_id = f"vec_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{document.id}"
        
        vectorization_logger.info(f"ğŸš€ å¼€å§‹å‘é‡åŒ–æ–‡æ¡£: {self.vectorization_session_id}")
        vectorization_logger.info(f"æ–‡æ¡£ä¿¡æ¯: ID={document.id}, æ–‡ä»¶å={document.filename}, è¯­è¨€={language}")
        
        result = VectorizationResult(success=False)
        start_time = time.time()
        
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
            if not self.models_loaded:
                vectorization_logger.error(f"âŒ æ¨¡å‹æœªåŠ è½½: {self.vectorization_session_id}")
                result.error = "å‘é‡åŒ–æ¨¡å‹æœªåŠ è½½"
                return result
            
            # æ­¥éª¤1: æ–‡æœ¬é¢„å¤„ç†
            preprocessed_text = self._preprocess_text(document.text, language, result)
            if not preprocessed_text:
                return result
            
            # æ­¥éª¤2: æ–‡æœ¬åˆ†æ®µ
            sections = self._segment_text(preprocessed_text, language, result)
            if not sections:
                return result
            
            # æ­¥éª¤3: ç”Ÿæˆå‘é‡
            vectors = self._generate_vectors(sections, result)
            if not vectors:
                return result
            
            # æ­¥éª¤4: åˆ›å»ºSectionè®°å½•
            section_records = self._create_section_records(document, sections, result)
            if not section_records:
                return result
            
            # æ­¥éª¤5: å­˜å‚¨åˆ°Qdrant
            storage_success = self._store_vectors_to_qdrant(document, section_records, vectors, result)
            if not storage_success:
                return result
            
            # å®Œæˆ
            result.success = True
            result.document_id = document.id
            result.sections_created = len(section_records)
            result.vectors_stored = len(vectors)
            result.processing_time = time.time() - start_time
            
            vectorization_logger.info(f"âœ… å‘é‡åŒ–å®Œæˆ: {self.vectorization_session_id}")
            vectorization_logger.info(f"ğŸ“Š ç»Ÿè®¡: {result.sections_created} æ®µè½, {result.vectors_stored} å‘é‡, è€—æ—¶ {result.processing_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            result.processing_time = time.time() - start_time
            result.error = str(e)
            vectorization_logger.error(f"âŒ å‘é‡åŒ–å¼‚å¸¸: {self.vectorization_session_id} - {str(e)}")
            
            import traceback
            vectorization_logger.error(f"å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
            
            return result
    
    def _preprocess_text(self, text: str, language: str, result: VectorizationResult) -> Optional[str]:
        """æ–‡æœ¬é¢„å¤„ç†"""
        vectorization_logger.debug(f"ğŸ“ å¼€å§‹æ–‡æœ¬é¢„å¤„ç†: {self.vectorization_session_id}")
        result.steps.append("å¼€å§‹æ–‡æœ¬é¢„å¤„ç†")
        
        try:
            if not text or not text.strip():
                vectorization_logger.warning(f"âš ï¸ æ–‡æœ¬ä¸ºç©º: {self.vectorization_session_id}")
                result.error = "æ–‡æœ¬å†…å®¹ä¸ºç©º"
                result.steps.append("æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: å†…å®¹ä¸ºç©º")
                return None
            
            # è®°å½•åŸå§‹æ–‡æœ¬ç»Ÿè®¡
            original_length = len(text)
            original_lines = len(text.split('\n'))
            vectorization_logger.debug(f"åŸå§‹æ–‡æœ¬: {original_length} å­—ç¬¦, {original_lines} è¡Œ")
            
            # åŸºç¡€æ¸…ç†
            cleaned_text = text.strip()
            
            # ç§»é™¤è¿‡å¤šçš„ç©ºç™½è¡Œ
            import re
            cleaned_text = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_text)
            
            # è®°å½•æ¸…ç†åçš„ç»Ÿè®¡
            cleaned_length = len(cleaned_text)
            cleaned_lines = len(cleaned_text.split('\n'))
            vectorization_logger.debug(f"æ¸…ç†åæ–‡æœ¬: {cleaned_length} å­—ç¬¦, {cleaned_lines} è¡Œ")
            
            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
            min_length = 50  # æœ€å°50å­—ç¬¦
            max_length = 1000000  # æœ€å¤§1MB
            
            if cleaned_length < min_length:
                vectorization_logger.warning(f"âš ï¸ æ–‡æœ¬è¿‡çŸ­: {self.vectorization_session_id} - {cleaned_length} < {min_length}")
                result.error = f"æ–‡æœ¬å†…å®¹è¿‡çŸ­ ({cleaned_length} å­—ç¬¦)"
                result.steps.append("æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: å†…å®¹è¿‡çŸ­")
                return None
            
            if cleaned_length > max_length:
                vectorization_logger.warning(f"âš ï¸ æ–‡æœ¬è¿‡é•¿: {self.vectorization_session_id} - {cleaned_length} > {max_length}")
                result.error = f"æ–‡æœ¬å†…å®¹è¿‡é•¿ ({cleaned_length} å­—ç¬¦)"
                result.steps.append("æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: å†…å®¹è¿‡é•¿")
                return None
            
            vectorization_logger.debug(f"âœ… æ–‡æœ¬é¢„å¤„ç†å®Œæˆ: {self.vectorization_session_id}")
            result.steps.append(f"æ–‡æœ¬é¢„å¤„ç†æˆåŠŸ: {cleaned_length} å­—ç¬¦")
            
            return cleaned_text
            
        except Exception as e:
            vectorization_logger.error(f"âŒ æ–‡æœ¬é¢„å¤„ç†å¼‚å¸¸: {self.vectorization_session_id} - {str(e)}")
            result.error = f"æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: {str(e)}"
            result.steps.append(f"æ–‡æœ¬é¢„å¤„ç†å¼‚å¸¸: {str(e)}")
            return None
    
    def _segment_text(self, text: str, language: str, result: VectorizationResult) -> Optional[List[str]]:
        """æ–‡æœ¬åˆ†æ®µ"""
        vectorization_logger.debug(f"âœ‚ï¸ å¼€å§‹æ–‡æœ¬åˆ†æ®µ: {self.vectorization_session_id}")
        result.steps.append("å¼€å§‹æ–‡æœ¬åˆ†æ®µ")
        
        try:
            # è°ƒç”¨åŸæœ‰çš„åˆ†æ®µé€»è¾‘
            from chat_with_your_data_api.embedding import prepare_text
            
            vectorization_logger.debug(f"ä½¿ç”¨è¯­è¨€æ¨¡å‹: {language}")
            
            # è·å–å¯¹åº”çš„spaCyæ¨¡å‹
            nlp = None
            if language == 'de' and self.nlp_de:
                nlp = self.nlp_de
            elif language == 'en' and self.nlp_en:
                nlp = self.nlp_en
            elif self.nlp_de:  # é»˜è®¤ä½¿ç”¨å¾·è¯­
                nlp = self.nlp_de
                vectorization_logger.warning(f"âš ï¸ è¯­è¨€æ¨¡å‹ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤å¾·è¯­æ¨¡å‹: {language}")
            else:
                vectorization_logger.error(f"âŒ æ— å¯ç”¨çš„spaCyæ¨¡å‹: {self.vectorization_session_id}")
                result.error = "æ— å¯ç”¨çš„è¯­è¨€æ¨¡å‹"
                result.steps.append("æ–‡æœ¬åˆ†æ®µå¤±è´¥: æ— å¯ç”¨æ¨¡å‹")
                return None
            
            # æ‰§è¡Œåˆ†æ®µ
            vectorization_logger.debug(f"ğŸ”„ æ‰§è¡Œæ–‡æœ¬åˆ†æ®µå¤„ç†...")
            sections = prepare_text(text, nlp)
            
            if not sections:
                vectorization_logger.warning(f"âš ï¸ åˆ†æ®µç»“æœä¸ºç©º: {self.vectorization_session_id}")
                result.error = "æ–‡æœ¬åˆ†æ®µå¤±è´¥"
                result.steps.append("æ–‡æœ¬åˆ†æ®µå¤±è´¥: ç»“æœä¸ºç©º")
                return None
            
            # è®°å½•åˆ†æ®µç»Ÿè®¡
            section_count = len(sections)
            avg_length = sum(len(s) for s in sections) / section_count if section_count > 0 else 0
            
            vectorization_logger.info(f"âœ… æ–‡æœ¬åˆ†æ®µå®Œæˆ: {self.vectorization_session_id} - {section_count} æ®µè½")
            vectorization_logger.debug(f"åˆ†æ®µç»Ÿè®¡: å¹³å‡é•¿åº¦ {avg_length:.1f} å­—ç¬¦")
            
            # è®°å½•æ¯ä¸ªæ®µè½çš„é•¿åº¦åˆ†å¸ƒ
            lengths = [len(s) for s in sections]
            min_len, max_len = min(lengths), max(lengths)
            vectorization_logger.debug(f"æ®µè½é•¿åº¦èŒƒå›´: {min_len} - {max_len} å­—ç¬¦")
            
            result.steps.append(f"æ–‡æœ¬åˆ†æ®µæˆåŠŸ: {section_count} æ®µè½")
            
            return sections
            
        except Exception as e:
            vectorization_logger.error(f"âŒ æ–‡æœ¬åˆ†æ®µå¼‚å¸¸: {self.vectorization_session_id} - {str(e)}")
            result.error = f"æ–‡æœ¬åˆ†æ®µå¤±è´¥: {str(e)}"
            result.steps.append(f"æ–‡æœ¬åˆ†æ®µå¼‚å¸¸: {str(e)}")
            return None
    
    def _generate_vectors(self, sections: List[str], result: VectorizationResult) -> Optional[List[np.ndarray]]:
        """ç”Ÿæˆå‘é‡"""
        vectorization_logger.debug(f"ğŸ§  å¼€å§‹ç”Ÿæˆå‘é‡: {self.vectorization_session_id}")
        result.steps.append("å¼€å§‹ç”Ÿæˆå‘é‡")
        
        try:
            section_count = len(sections)
            vectorization_logger.info(f"ğŸ“Š ç”Ÿæˆ {section_count} ä¸ªæ®µè½çš„å‘é‡")
            
            # æ‰¹é‡ç”Ÿæˆå‘é‡
            vectorization_logger.debug("ğŸ”„ SentenceTransformerç¼–ç ä¸­...")
            start_time = time.time()
            
            vectors = self.sentence_model.encode(sections, show_progress_bar=False, convert_to_numpy=True)
            
            encoding_time = time.time() - start_time
            vectorization_logger.info(f"âœ… å‘é‡ç”Ÿæˆå®Œæˆ: {encoding_time:.2f}ç§’")
            
            # æ£€æŸ¥å‘é‡ç»´åº¦
            if len(vectors) != section_count:
                vectorization_logger.error(f"âŒ å‘é‡æ•°é‡ä¸åŒ¹é…: {self.vectorization_session_id} - {len(vectors)} != {section_count}")
                result.error = "å‘é‡ç”Ÿæˆæ•°é‡é”™è¯¯"
                result.steps.append("å‘é‡ç”Ÿæˆå¤±è´¥: æ•°é‡ä¸åŒ¹é…")
                return None
            
            # è®°å½•å‘é‡ç»Ÿè®¡
            if len(vectors) > 0:
                vector_dim = vectors[0].shape[0]
                vectorization_logger.debug(f"å‘é‡ç»´åº¦: {vector_dim}")
                vectorization_logger.debug(f"å‘é‡æ•°æ®ç±»å‹: {vectors[0].dtype}")
                
                # æ£€æŸ¥å‘é‡æœ‰æ•ˆæ€§
                for i, vector in enumerate(vectors):
                    if np.isnan(vector).any():
                        vectorization_logger.warning(f"âš ï¸ å‘é‡ {i} åŒ…å«NaNå€¼")
                    if np.isinf(vector).any():
                        vectorization_logger.warning(f"âš ï¸ å‘é‡ {i} åŒ…å«æ— ç©·å€¼")
            
            result.steps.append(f"å‘é‡ç”ŸæˆæˆåŠŸ: {len(vectors)} ä¸ªå‘é‡")
            
            return vectors
            
        except Exception as e:
            vectorization_logger.error(f"âŒ å‘é‡ç”Ÿæˆå¼‚å¸¸: {self.vectorization_session_id} - {str(e)}")
            result.error = f"å‘é‡ç”Ÿæˆå¤±è´¥: {str(e)}"
            result.steps.append(f"å‘é‡ç”Ÿæˆå¼‚å¸¸: {str(e)}")
            return None
    
    def _create_section_records(self, document, sections: List[str], result: VectorizationResult) -> Optional[List]:
        """åˆ›å»ºSectionè®°å½•"""
        vectorization_logger.debug(f"ğŸ“ åˆ›å»ºSectionè®°å½•: {self.vectorization_session_id}")
        result.steps.append("å¼€å§‹åˆ›å»ºSectionè®°å½•")
        
        try:
            from chat_with_your_data_api.models import Section
            from chat_with_your_data_api.serializers import SectionSerializer
            
            section_records = []
            
            for i, section_text in enumerate(sections):
                vectorization_logger.debug(f"åˆ›å»ºSection {i+1}/{len(sections)}")
                
                section_data = {
                    'document': document.id,
                    'section_text': section_text,
                    'section_number': i + 1
                }
                
                serializer = SectionSerializer(data=section_data)
                
                if serializer.is_valid():
                    section_record = serializer.save()
                    section_records.append(section_record)
                    vectorization_logger.debug(f"âœ… Section {i+1} åˆ›å»ºæˆåŠŸ: ID {section_record.id}")
                else:
                    vectorization_logger.error(f"âŒ Section {i+1} éªŒè¯å¤±è´¥: {serializer.errors}")
                    result.error = f"Sectionè®°å½•éªŒè¯å¤±è´¥: {serializer.errors}"
                    result.steps.append(f"Sectionè®°å½•åˆ›å»ºå¤±è´¥: éªŒè¯é”™è¯¯")
                    return None
            
            vectorization_logger.info(f"âœ… Sectionè®°å½•åˆ›å»ºå®Œæˆ: {self.vectorization_session_id} - {len(section_records)} è®°å½•")
            result.steps.append(f"Sectionè®°å½•åˆ›å»ºæˆåŠŸ: {len(section_records)} è®°å½•")
            
            return section_records
            
        except Exception as e:
            vectorization_logger.error(f"âŒ Sectionè®°å½•åˆ›å»ºå¼‚å¸¸: {self.vectorization_session_id} - {str(e)}")
            result.error = f"Sectionè®°å½•åˆ›å»ºå¤±è´¥: {str(e)}"
            result.steps.append(f"Sectionè®°å½•åˆ›å»ºå¼‚å¸¸: {str(e)}")
            return None
    
    def _store_vectors_to_qdrant(self, document, section_records: List, vectors: List[np.ndarray], result: VectorizationResult) -> bool:
        """å­˜å‚¨å‘é‡åˆ°Qdrant"""
        qdrant_logger.info(f"ğŸ—„ï¸ å¼€å§‹å­˜å‚¨å‘é‡åˆ°Qdrant: {self.vectorization_session_id}")
        result.steps.append("å¼€å§‹å­˜å‚¨å‘é‡åˆ°Qdrant")
        
        try:
            from chat_with_your_data_api.qdrant import get_client
            
            # è·å–Qdrantå®¢æˆ·ç«¯
            client = get_client()
            if not client:
                qdrant_logger.error(f"âŒ Qdrantå®¢æˆ·ç«¯è·å–å¤±è´¥: {self.vectorization_session_id}")
                result.error = "Qdrantè¿æ¥å¤±è´¥"
                result.steps.append("å‘é‡å­˜å‚¨å¤±è´¥: è¿æ¥é”™è¯¯")
                return False
            
            # å‡†å¤‡å­˜å‚¨æ•°æ®
            user = document.user
            if not user or not hasattr(user, 'auth0_id'):
                qdrant_logger.error(f"âŒ ç”¨æˆ·ä¿¡æ¯ç¼ºå¤±: {self.vectorization_session_id}")
                result.error = "ç”¨æˆ·ä¿¡æ¯ç¼ºå¤±"
                result.steps.append("å‘é‡å­˜å‚¨å¤±è´¥: ç”¨æˆ·ä¿¡æ¯é”™è¯¯")
                return False
            
            # æå–é›†åˆåç§°
            collection_name = user.auth0_id.split("|")[1] if "|" in user.auth0_id else user.auth0_id
            qdrant_logger.info(f"ç›®æ ‡é›†åˆ: {collection_name}")
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collection_exists = self._ensure_collection_exists(client, collection_name, result)
            if not collection_exists:
                return False
            
            # æ‰¹é‡å­˜å‚¨å‘é‡
            stored_count = 0
            for i, (section_record, vector) in enumerate(zip(section_records, vectors)):
                try:
                    # æ„é€ ç‚¹æ•°æ®
                    point_id = section_record.id
                    payload = {
                        'document_id': document.id,
                        'section_id': section_record.id,
                        'filename': document.filename,
                        'section_text': section_record.section_text[:1000],  # é™åˆ¶è½½è·å¤§å°
                        'section_number': section_record.section_number
                    }
                    
                    # å­˜å‚¨å•ä¸ªå‘é‡
                    client.upsert(
                        collection_name=collection_name,
                        points=[{
                            'id': point_id,
                            'vector': vector.tolist(),
                            'payload': payload
                        }]
                    )
                    
                    stored_count += 1
                    qdrant_logger.debug(f"âœ… å‘é‡ {i+1} å­˜å‚¨æˆåŠŸ: Point ID {point_id}")
                    
                except Exception as e:
                    qdrant_logger.error(f"âŒ å‘é‡ {i+1} å­˜å‚¨å¤±è´¥: {str(e)}")
                    # ç»§ç»­å¤„ç†å…¶ä»–å‘é‡
            
            if stored_count == 0:
                qdrant_logger.error(f"âŒ æ‰€æœ‰å‘é‡å­˜å‚¨å¤±è´¥: {self.vectorization_session_id}")
                result.error = "å‘é‡å­˜å‚¨å…¨éƒ¨å¤±è´¥"
                result.steps.append("å‘é‡å­˜å‚¨å¤±è´¥: å…¨éƒ¨å¤±è´¥")
                return False
            
            qdrant_logger.info(f"âœ… å‘é‡å­˜å‚¨å®Œæˆ: {self.vectorization_session_id} - {stored_count}/{len(vectors)} æˆåŠŸ")
            result.steps.append(f"å‘é‡å­˜å‚¨æˆåŠŸ: {stored_count}/{len(vectors)}")
            
            return stored_count > 0
            
        except Exception as e:
            qdrant_logger.error(f"âŒ Qdrantå­˜å‚¨å¼‚å¸¸: {self.vectorization_session_id} - {str(e)}")
            result.error = f"å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}"
            result.steps.append(f"å‘é‡å­˜å‚¨å¼‚å¸¸: {str(e)}")
            return False
    
    def _ensure_collection_exists(self, client, collection_name: str, result: VectorizationResult) -> bool:
        """ç¡®ä¿é›†åˆå­˜åœ¨"""
        qdrant_logger.debug(f"ğŸ” æ£€æŸ¥é›†åˆ: {collection_name}")
        
        try:
            from qdrant_client.models import Distance, VectorParams
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = client.get_collections()
            existing_collections = [col.name for col in collections.collections]
            
            if collection_name in existing_collections:
                qdrant_logger.debug(f"âœ… é›†åˆå·²å­˜åœ¨: {collection_name}")
                return True
            
            # åˆ›å»ºæ–°é›†åˆ
            qdrant_logger.info(f"ğŸ”¨ åˆ›å»ºæ–°é›†åˆ: {collection_name}")
            
            client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=384,  # SentenceTransformerè¾“å‡ºç»´åº¦
                    distance=Distance.COSINE
                )
            )
            
            qdrant_logger.info(f"âœ… é›†åˆåˆ›å»ºæˆåŠŸ: {collection_name}")
            return True
            
        except Exception as e:
            qdrant_logger.error(f"âŒ é›†åˆæ“ä½œå¤±è´¥: {collection_name} - {str(e)}")
            result.error = f"é›†åˆæ“ä½œå¤±è´¥: {str(e)}"
            return False


# å…¨å±€å‘é‡åŒ–å™¨å®ä¾‹
_vectorizer = None

def get_vectorizer() -> EnhancedVectorizer:
    """è·å–å‘é‡åŒ–å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _vectorizer
    if _vectorizer is None:
        _vectorizer = EnhancedVectorizer()
    return _vectorizer


def vectorize_document_enhanced(document, language: str = 'de') -> VectorizationResult:
    """å¢å¼ºç‰ˆæ–‡æ¡£å‘é‡åŒ–å‡½æ•° - å¤–éƒ¨æ¥å£"""
    vectorizer = get_vectorizer()
    return vectorizer.vectorize_document(document, language) 